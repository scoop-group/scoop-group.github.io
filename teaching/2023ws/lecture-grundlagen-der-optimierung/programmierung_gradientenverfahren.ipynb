{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmierübung 1 zu *Grundlagen der Optimierung* (WS2023)\n",
    "\n",
    "## Einführung\n",
    "\n",
    "### Verantwortlich\n",
    "* Dr. Evelyn Herberg\n",
    "* M.Sc. Masoumeh Hashemi\n",
    "* B.Sc. Viktor Stein\n",
    "\n",
    "### Zielsetzung\n",
    "Das Ziel dieses *Jupyter Notebooks* ist es, Ihnen das Verhalten der im Kapitel 1 des Skripts vorgestellten Algorithmen zur Lösung allgemeiner, unrestringierter Optimierungsprobleme nahezubringen.\n",
    "Wir werden den Einfluss der *Vorkonditionierer* im *Gradientenverfahren* untersuchen.\n",
    "\n",
    "### Zur Nutzung des Notebooks\n",
    "Die numerische Umsetzung der Verfahren und die graphische Visualisierung typischer Ergebnisse ist ein essentieller Baustein auf dem Weg zu einem ausgereiften Verständnis der Algorithmen.\n",
    "Um programmiertechnische Schwierigkeiten weitestgehend auszuschließen, haben wir einiges an Code für Sie vorbereitet.\n",
    "An den Schlüsselstellen der jeweiligen Implementierungen wurde der lauffähige Code durch auskommentierte Blöcke der Art\n",
    "```python\n",
    "### TODO BEGIN ###\n",
    "# Compute the preconditioned gradient and the square of its (preconditioner-induced) norm \n",
    "# gradient = ...\n",
    "# norm2_gradient = ...\n",
    "### TODO END ###\n",
    "```\n",
    "ersetzt, in denen Sie zwischen `### TODO BEGIN ###` und `### TODO END ###` die entsprechenden Anweisungen Variablen (in diesem Beispiel die Berechnung von `gradient` und `norm2_gradient`) entsprechend des Kommentars ausführen, um Lauffähigkeit wieder herzustellen.\n",
    "Welche Berechnungen und Auswertungen an den jeweiligen Stellen benötigt werden, können Sie im Skript nachlesen.\n",
    "Sie können natürlich, bevor Sie genau diese vorbenannten Variablen schreiben, auch eigene Variablen beschreiben.\n",
    "\n",
    "Wenn sie den Code vervollständigt haben, werden Sie an geeigneter Stelle um die Interpretation der Ergebnisse gebeten.\n",
    "In den entsprechenden Zellen ersetzen Sie \"**TODO Ihre Antwort hier**\" mit ihrer Antwort.\n",
    "\n",
    "## Das Gradientenverfahren für quadratische Zielfunktionen\n",
    "In diesem Abschnitt wollen wir das Verhalten des *Gradientenverfahrens* bei *quadratischen Zielfunktionen* mit symmetrischem, positiv definitem quadratischen Teil bei Verwendung der *exakten Schrittweitenbestimmung* untersuchen.\n",
    "Wir wollen hier insbesondere den Einfluss des Vorkonditionierers auf die Konvergenzgeschwindigkeit untersuchen, denn\n",
    "wie wir wissen kann man in diesem Fall die *q-lineare Konvergenz* des Gradientenverfahrens gegen den eindeutigen Minimierer des Problems beweisen und kann sogar den Vorfaktor explizit in Abhängigkeit von der *verallgemeinerten Konditionszahl* angeben.\n",
    "\n",
    "### Implementierung des Gradientenverfahrens\n",
    "In der nachfolgenden Zelle finden Sie die Funktion zum Gradientenverfahren mit den fehlenden Berechnungen.\n",
    "\n",
    "**Aufgabe:** Vervollständigen Sie den Code und führen Sie die Zelle aus.\n",
    "\n",
    "Beachten Sie, dass sowohl die verwendete Schrittweitenberechnung als auch die Wahl des Vorkonditionierers - mit gutem Grund - durch den/die AnwenderIn explizit vorgegeben werden muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This module implements the preconditioned gradient scheme.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, x0, step_length_rule, preconditioner, parameters = {}):\n",
    "  \"\"\" \n",
    "  Solve an unconstrained minimization problem using the preconditioned\n",
    "  gradient descent method.\n",
    "\n",
    "  Accepts:  \n",
    "                   f: the objective function to be minimized\n",
    "                  x0: the initial guess (list or numpy array with ndim == 1)\n",
    "    step_length_rule: a step length computation function \n",
    "      preconditioner: a symmetric positive definite matrix (numpy array with ndim == 2)\n",
    "          parameters: optional parameters (dictionary);\n",
    "                      the following key/value pairs are evaluated:\n",
    "                                [\"atol_x\"]: absolute stopping tolerance for the norm of updates in x\n",
    "                                [\"rtol_x\"]: relative stopping tolerance for the norm of updates in x\n",
    "                                [\"atol_f\"]: absolute stopping tolerance for the progress in the values of f\n",
    "                                [\"rtol_f\"]: relative stopping tolerance for the progress in the values of f\n",
    "                            [\"atol_gradf\"]: absolute stopping tolerance for the norm of the gradient of f\n",
    "                            [\"rtol_gradf\"]: relative stopping tolerance for the norm of the gradient of f\n",
    "                        [\"max_iterations\"]: maximum number of iterations\n",
    "                             [\"verbosity\"]: \"verbose\" or \"quiet\"\n",
    "                          [\"keep_history\"]: whether or not to store the iteration history (True or False) \n",
    "                      Here 'norm' refers to the preconditioner-induced norm.\n",
    "    \n",
    "  Returns: \n",
    "              result: a dictionary containing \n",
    "                          solution: final iterate\n",
    "                          function: the final iterate's objective value \n",
    "                          gradient: the final iterate's objective gradient value\n",
    "                     norm_gradient: preconditioner-induced norm of final objective gradient \n",
    "                              iter: number of iterations performed\n",
    "                          exitflag: flag encoding why the algorithm terminated\n",
    "                                   0: stopping tolerance described by atol_x, rtol_x, atol_f, rtol_f reached\n",
    "                                   1: stopping tolerance described by atol_gradf and rtol_gradf reached\n",
    "                                   2: maximum number of iterations reached\n",
    "\n",
    "                       history: a dictionary for the history of the run containing\n",
    "                                          iterates: the iterates x\n",
    "                                  objective_values: the values of the objective function\n",
    "                                    gradient_norms: the norms of the objective function gradient \n",
    "                                     steps_lengths: the step lengths chosen by the step length rule\n",
    "  \"\"\"\n",
    "  # Define computation of the squared preconditioner norm\n",
    "  def norm2(d): return d.dot(preconditioner.dot(d))\n",
    "\n",
    "  # Define an output function that will be used to print information on the state of the iteration\n",
    "  def print_header(): \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(' ITER          OBJ    NORM_GRAD    NORM_CORR     OBJ_CHNG           ')\n",
    "    print('--------------------------------------------------------------------')\n",
    "  \n",
    "  # Define exitflags messages that will be printed when the algorithm terminates\n",
    "  exitflag_messages = [\n",
    "      'Relative and absolute tolerances on the norm of the update and the descent of the objective are satisfied.',\n",
    "      'Relative and absolute tolerances on the norm of the gradient are satisfied.',\n",
    "      'Maximum number of optimization steps is reached.',\n",
    "      ]\n",
    "  \n",
    "  # Get the algorithmic parameters, using defaults if missing\n",
    "  atol_x = parameters.get(\"atol_x\", 1e-6)\n",
    "  rtol_x = parameters.get(\"rtol_x\", 1e-6)\n",
    "  atol_f = parameters.get(\"atol_f\", 1e-6)\n",
    "  rtol_f = parameters.get(\"rtol_f\", rtol_x**2)\n",
    "  atol_gradf = parameters.get(\"atol_gradf\", 1e-6)\n",
    "  rtol_gradf = parameters.get(\"rtol_gradf\", 1e-6)\n",
    "  max_iterations = parameters.get(\"max_iterations\", 1e3)\n",
    "  verbosity = parameters.get(\"verbosity\", \"quiet\")\n",
    "  keep_history = parameters.get(\"keep_history\", False)\n",
    "\n",
    "  # Initialize the iterates, counters etc.\n",
    "  x = x0\n",
    "  iter = 0\n",
    "  exitflag = None\n",
    "  \n",
    "  # Initialize dummy values pertaining to the previous iterate\n",
    "  x_old = np.full(x0.shape, np.inf)\n",
    "  function_value_old = np.inf\n",
    "\n",
    "  # Prepare a dictionary to store the history\n",
    "  if keep_history:\n",
    "    history = {\n",
    "      \"iterates\" : [],\n",
    "      \"objective_values\" : [],\n",
    "      \"gradient_norms\" : [],\n",
    "      \"step_lengths\" : []\n",
    "      }\n",
    "  \n",
    "  # Perform gradient descent steps until one of the termination criteria is met\n",
    "  while exitflag is None:\n",
    "    # Record the current iterate\n",
    "    if keep_history: history[\"iterates\"].append(x)\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose':\n",
    "      if (iter%10 == 0): print_header()\n",
    "      print(' %4d  ' % (iter), end = '')\n",
    "            \n",
    "    # Stop when the maximum number of iterations has been reached\n",
    "    if iter >= max_iterations:\n",
    "      exitflag = 2\n",
    "      break\n",
    "    \n",
    "    # Compute the function value and derivative at current iterate\n",
    "    values = f(x, derivatives = [True, True, False])\n",
    "    function_value = values[\"function\"]\n",
    "    derivative = values[\"derivative\"]\n",
    "    \n",
    "    # Record the current value of the objective\n",
    "    if keep_history: history[\"objective_values\"].append(function_value)\n",
    "        \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  ' % (function_value), end = '')\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Compute the preconditioned gradient and the square of its (preconditioner-induced) norm \n",
    "    # gradient = ...\n",
    "    # norm2_gradient = ...\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Check the computed norm square for positivity\n",
    "    if norm2_gradient < 0:\n",
    "      raise ValueError('Your preconditioner appears not to be positive definite.')\n",
    "    else:\n",
    "      norm_gradient = np.sqrt(norm2_gradient)\n",
    "    \n",
    "    # Record the current norm of the gradient\n",
    "    if keep_history: history[\"gradient_norms\"].append(norm_gradient)\n",
    "\n",
    "    # Remember the norm of the initial gradient\n",
    "    if (iter == 0): initial_norm_gradient = norm_gradient\n",
    "        \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  ' % (norm_gradient), end = '')\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Stop when the stopping tolerance on the norm of the gradient has been reached\n",
    "    # if ...\n",
    "      exitflag = 1\n",
    "      break\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Evaluate the norm of the update step\n",
    "    norm_delta_x = np.sqrt(norm2(x - x_old))\n",
    "    \n",
    "    # Evaluate the change in the objective function values\n",
    "    delta_f = function_value_old - function_value\n",
    "    \n",
    "    # Evaluate the reference values for relative tolerances\n",
    "    abs_function_value_old = np.abs(function_value_old)\n",
    "    norm_x_old = np.sqrt(norm2(x_old))\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  %11.4e' % (norm_delta_x, -delta_f))\n",
    "    \n",
    "    # Stop when the stopping tolerance on the change in the objective and the\n",
    "    # norm of the update step have been reached\n",
    "    if (delta_f < atol_f + rtol_f * abs_function_value_old) and\\\n",
    "      (norm_delta_x < atol_x + rtol_x * norm_x_old):\n",
    "      exitflag = 0\n",
    "      break\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Set the update direction\n",
    "    # d = ...\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Prepare the line search function, using the function values of the\n",
    "    # objective and its derivatives and the chain rule\n",
    "    def phi(t, derivatives):\n",
    "      values = f(x + t * d, derivatives)\n",
    "      if derivatives[1]:\n",
    "        values[\"derivative\"] = values[\"derivative\"].dot(d)\n",
    "      if derivatives[2]:\n",
    "        values[\"Hessian\"] = d.dot(values[\"Hessian\"].dot(d))\n",
    "      return values\n",
    "    \n",
    "    # Prepare some data to pass down to the step length computation rule\n",
    "    reusables = {\n",
    "      \"phi0\" : function_value,\n",
    "      \"dphi0\" : -norm2_gradient\n",
    "      }\n",
    "    \n",
    "    # Evaluate the step length t using the step length rule\n",
    "    t, t_exitflag = step_length_rule(phi, reusables)\n",
    "    \n",
    "    # Check whether of not the step length was computed succesfully\n",
    "    if t_exitflag: raise AssertionError('Step length was not computed succesfully.')\n",
    "    \n",
    "    # Record the chosen step length\n",
    "    if keep_history: history[\"step_lengths\"].append(t)\n",
    "    \n",
    "    # Save the current iterate and associated function value for the next iteration\n",
    "    x_old = x\n",
    "    function_value_old = function_value\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Update the iterate and increase the counter\n",
    "    # x = ...\n",
    "    # iter = ...\n",
    "    ### TODO END ###\n",
    "\n",
    "  # Dump some output\n",
    "  if verbosity == 'verbose':\n",
    "    print('\\n\\nThe gradient descent method exiting with flag %d.\\n' %(exitflag) + str(exitflag_messages[exitflag])+'\\n' )\n",
    "  \n",
    "  # Create and populate the result to be returned\n",
    "  result = {\n",
    "    \"solution\" : x,\n",
    "    \"function\" : function_value,\n",
    "    \"gradient\" : gradient,\n",
    "    \"norm_gradient\" : norm_gradient,\n",
    "    \"iter\" : iter,\n",
    "    \"exitflag\" : exitflag\n",
    "    }\n",
    "\n",
    "  # Assign the history to the result if required\n",
    "  if keep_history:\n",
    "    result[\"history\"] = history\n",
    "      \n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementierung der exakten quadratischen Schrittweite\n",
    "\n",
    "Um das oben implementierte Gradientenverfahren im Rahmen dieses Abschnitts untersuchen zu können, benötigen wir lediglich noch die Schrittweitensteuerung.\n",
    "\n",
    "**Aufgabe:** Vervollständigen Sie den Code in der nächsten Zelle und führen Sie die Zelle aus.\n",
    "\n",
    "Beachten Sie, dass das Interface der Schrittweitenbestimmung lediglich den *Schnitt* $\\varphi$ der Funktion $f$ übergeben bekommt. \n",
    "Sie müssen also den Ausdruck in Abhängigkeit von dem quadratischen Teil $Q$ der Zielfunktion und dem Vorkonditionierer $M$ aus dem Skript durch durch $\\varphi$-Terme ausdrücken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This module implements the exact step length computation for quadratic objective functionals in the gradient scheme\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def exact_step_length_quadratic(phi, reusables = {}):\n",
    "  \"\"\" \n",
    "  Compute the exact minimizer of a one-dimensional function assumed\n",
    "  to be a quadratic polynomial.\n",
    "\n",
    "  Accepts: \n",
    "           phi: evaluates the function the line search is performed on\n",
    "     reusables: additional information that may be provided to the method (dictionary);\n",
    "                the following key/value pairs are evaluated:\n",
    "                  [\"phi0\"]: the value of phi at t = 0 (scalar)\n",
    "                  [\"dphi0\"]: the value of the derivative of phi at t = 0 (scalar)\n",
    "\n",
    "  Returns:\n",
    "            t: the step length minimizing phi (provided it is quadratic)\n",
    "     exitflag: 0\n",
    "  \"\"\"\n",
    "\n",
    "  # The exact minimizer of phi is evaluated using the following data:\n",
    "  # phi(0), phi'(0), phi(1). Evaluate this data if it is not provided.\n",
    "  phi0 = reusables.get(\"phi0\", phi(0, derivatives = [True, False, False])[\"function\"]) or\\\n",
    "    phi(0, derivatives = [True,False,False])[\"function\"]\n",
    "  dphi0 = reusables.get(\"dphi0\", phi(0, derivatives=[False, True, False])[\"derivative\"]) or\\\n",
    "    phi(0, derivatives = [False,True,False])[\"derivative\"]\n",
    "  if dphi0 >= 0:\n",
    "    raise(InputError('The function phi is expected to be decreasing at zero..'))\n",
    "  phi1 = phi(1,derivatives = [True, False, False])[\"function\"]\n",
    "  \n",
    "  ### TODO BEGIN ###\n",
    "  # Evaluate the exact step length\n",
    "  # t = ...\n",
    "  ### TODO END ###\n",
    "  \n",
    "  \n",
    "  # Check if the step length is in fact positive, i.e., whether phi has positive curvature.\n",
    "  if t < 0.0:\n",
    "    raise ValueError('The step length computation yields a negative step length.')\n",
    "  else:\n",
    "    return t, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchung des Verhaltens der Iterierten\n",
    "\n",
    "An dieser Stelle können wir bereits mit unserer Untersuchung beginnen.\n",
    "Das Skript in der nächsten Zelle soll das implementierte Gradientenverfahren mit verschiedenen Vorkonditionierern auf ein quadratisches Problem anwenden und die verallgemeinerten Konditionszahlen berechnen.\n",
    "Der Output entspricht dem Status des Gradientenverfahren in den jeweiligen Iterationen.\n",
    "Das Skript ist lauffähig und verwendet die euklidische Vorkonditionierung.\n",
    "\n",
    "**Aufgabe:** Implementieren Sie zu dem euklidischen Vorkonditionierer mindestens drei weitere Vorkonditionierer.\n",
    "Achten Sie darauf, auch einen zu implementieren, der zu langsamerer Konvergenz als der euklidische, führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from objective_functions import *\n",
    "from scipy.linalg import eigh\n",
    "from visualization_functions import *\n",
    "\n",
    "# Create data\n",
    "Q = np.array([[10.0, -2.0], [-2.0, 1.0]])\n",
    "c = np.array([2.0, 0.0])\n",
    "gamma = 0.0\n",
    "f = lambda u, derivatives: quadratic_function(u, derivatives, Q, c, gamma)\n",
    "x0 = np.array([1.0, 5.0])\n",
    "\n",
    "# Construct step length rule\n",
    "exact_step_length_rule_quadratic = lambda phi, reusables: exact_step_length_quadratic(phi, reusables)\n",
    "\n",
    "# Construct the preconditioners\n",
    "preconditioners = [(np.identity(len(x0)), \"Identity\"), # Identity                  \n",
    "                   ### TODO BEGIN ###\n",
    "                   # Add three preconditioners of the format\n",
    "                   # (matrix, \"Label for plotting\"),\n",
    "                   ### TODO END ###\n",
    "                  ] \n",
    "\n",
    "# Set gradient scheme parameters\n",
    "optimization_parameters = {\n",
    "\"atol_x\" : 1e-7,\n",
    "\"rtol_x\" : 1e-7,\n",
    "\"atol_f\" : 1e-7,\n",
    "\"rtol_f\" : 1e-14,\n",
    "\"max_iterations\" : 1e4,\n",
    "\"c\" : 10,\n",
    "\"verbosity\" : \"verbose\",\n",
    "\"keep_history\" : True\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "labels = []\n",
    "generalized_condition_numbers = []\n",
    "\n",
    "# Solve problem for all choices of the preconditioner\n",
    "for preconditioner, label in preconditioners:\n",
    "  outputs.append(gradient_descent(f, x0, exact_step_length_rule_quadratic, \n",
    "                                 preconditioner, optimization_parameters))\n",
    "  labels.append(label)\n",
    "  generalized_eigenvalues = eigh(Q, preconditioner, eigvals_only = True)\n",
    "  generalized_condition_numbers.append(generalized_eigenvalues[-1] / generalized_eigenvalues[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls der Code in der vorherigen Zelle durchgelaufen ist, können wir nun die Höhenlinien der Zielfunktion und die von den verschiedenen Durchläufen besuchten Iterierten plotten.\n",
    "Führen Sie dazu die folgende Zelle aus, Sie müssen nichts ergänzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot history in iterate space\n",
    "plot_2d_iterates_contours(f, list(out[\"history\"] for out in outputs), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Beschreiben Sie den Einfluss der von Ihnen gewählten Vorkonditionierer auf den Verlauf der Iterationen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Ihre Antwort hier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchung des Konvergenzverhaltens\n",
    "\n",
    "Nun wollen wir uns noch das Konvergenzverhalten im Sinne der Energienorm des Fehlers ansehen.\n",
    "Für unser obiges Problem können wir nämlich den exakten Minimierer direkt bestimmen. \n",
    "Mit den Aussagen im Konvergenzbeweis können wir nun die (quadrierte) Energienorm des Fehlers in jeder Iteration gegen eine in dem Vorfaktor $\\frac{k-1}{k+1}$ exponentiell fallende Folge abschätzen und als obere Schranke plotten.\n",
    "\n",
    "**Aufgabe: Berechnen Sie den exakten Minimierer mittels der Bedingungen erster Ordnung.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TODO BEGIN ###\n",
    "# Compute the actual solution of the problem and its function value for verification\n",
    "# x_opt = ...\n",
    "f_opt = f(x_opt, derivatives = [True, False, False])[\"function\"]\n",
    "### TODO END ###\n",
    "\n",
    "# Plot functional value differences (approximation of error energy norm)\n",
    "plot_f_val_diffs(list(out[\"history\"] for out in outputs), \n",
    "                [f_opt] * len(outputs),\n",
    "                labels,\n",
    "                generalized_condition_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Beschreiben Sie kurz das beobachtete Verhalten in diesem Plot und ob er mit den Ergebnissen aus der Vorlesung konsistent ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Ihre Antwort hier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchung der Schrittweiten und der Gradientennormen\n",
    "\n",
    "Als Referenz für Sie, plotten wir mit der nächsten Zelle noch einmal die gewählten Schrittweiten und die Vorkonditionierernorm der Gradienten.\n",
    "Führen Sie dafür die Zelle einfach aus.\n",
    "Beachten Sie das stark zappelige Verhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_step_sizes(list(out[\"history\"] for out in outputs), labels)\n",
    "\n",
    "plot_grad_norms(list(out[\"history\"] for out in outputs), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Gradientenverfahren für nichtquadratische Funktionen\n",
    "\n",
    "Quadratische Funktionen sind in der unbeschränkten Optimierung die einfachsten, sinnvoll zu untersuchenden Funktionen. \n",
    "Für Optimierungsaufgaben mit allgemeineren Funktionen ist das Verhalten des Gradientenverfahrens nicht mehr so leicht zu analysieren und vorherzusagen.\n",
    "Außerdem ist die Wahl eines \"nützlichen\" Vorkonditionierers entsprechend schwieriger.\n",
    "Wir werden jetzt das Verhalten des Gradientenverfahrens mit verschiedenen Vorkonditionierern und Armijo-Backtracking anhand der [Funktion von *Himmelblau*](https://en.wikipedia.org/wiki/Himmelblau%27s_function) - insbesondere hinsichtlich des Verhaltens bzgl. lokaler Minimierer untersuchen.\n",
    "\n",
    "\n",
    "### Armijo Backtracking Strategie\n",
    "\n",
    "Das Gradientenverfahren von oben können wir natürlich weiterverwenden.\n",
    "Lediglich die Schrittweitensteuerung müssen wir austauschen, denn für allgemeine Funktionen finden wir keinen analytischen Ausdruck für eine exakte Schrittweite.\n",
    "Wir werden stattdessen die im Skript beschriebene Backtracking Strategie mit der Armijo Regel implementieren.\n",
    "\n",
    "**Aufgaben:** Vervollständigen Sie den Code zur Armijo Schrittweitensteuerung in der nächsten Zelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def armijo_backtracking(phi, reusables = {}, parameters = {}):\n",
    "  \"\"\"\n",
    "  Compute a step length t via backtracking satisfying the Armijo condition \n",
    "  for the function phi, i.e.,\n",
    "\n",
    "    phi(t) <= phi(0) + sigma * t * dphi(0) \n",
    "\n",
    "  where dphi(0) is the derivative of phi at t = 0.\n",
    "\n",
    "  Accepts: \n",
    "           phi: evaluates the function the line search is performed on\n",
    "     reusables: additional information that may be provided to the method (dictionary);\n",
    "                the following key/value pairs are evaluated:\n",
    "                   [\"phi0\"]: the value of phi at t = 0 (scalar)\n",
    "                  [\"dphi0\"]: the value of the derivative of phi at t = 0 (scalar)\n",
    "\n",
    "  Returns:\n",
    "            t: the step length minimizing phi (provided it is quadratic)\n",
    "     exitflag: flag encoding why the line search terminated\n",
    "                 0: success\n",
    "                 1: maximum number of iterations reached\n",
    "                 2: trial step length became too small\n",
    "  \"\"\"\n",
    "\n",
    "  def print_header():\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(' ARMIJO:     ITER          STEP      OBJCHNG           ')\n",
    "    print('--------------------------------------------------------------------')\n",
    "  \n",
    "  # Get the line search parameters, using defaults if missing\n",
    "  sigma = parameters.get(\"sigma\", 0.01)\n",
    "  beta = parameters.get(\"beta\", 0.5)\n",
    "  initial_t = parameters.get(\"initial_step_length\", 1.0)\n",
    "  verbosity = parameters.get(\"verbosity\", \"quiet\")\n",
    "  max_iterations = parameters.get(\"max_iterations\", 1e4)\n",
    "    \n",
    "  # Extract or compute required data for checking armijo condition\n",
    "  phi0 = reusables.get(\"phi0\", phi(0, derivatives = [True, False, False])[\"function\"]) or\\\n",
    "    phi(0, derivatives[True,False,False])[\"function\"]\n",
    "  dphi0 = reusables.get(\"dphi0\", phi(0, derivatives = [False, True, False])[\"derivative\"]) or\\\n",
    "    phi(0, derivatives[False,True,False])[\"derivative\"]\n",
    "  \n",
    "  if dphi0 >= 0:\n",
    "    raise(InputError('The function phi is expected to be decreasing at zero..'))\n",
    "  \n",
    "  # Initialize the step length and counter\n",
    "  t = initial_t\n",
    "  iter = 0\n",
    "  exitflag = None\n",
    "    \n",
    "  # Perform the backtracking search until one of the termination criteria is met\n",
    "  while exitflag is None:\n",
    "\n",
    "    # Evaluate the value of phi at the current trial step length and the amount of descent\n",
    "    phi_trial = phi(t, derivatives = [True, False, False])[\"function\"]\n",
    "    delta_phi = phi_trial - phi0\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose':\n",
    "      if (iter%10 == 0): print_header()\n",
    "      print('             %4d   %11.4e  %11.4e  \\n' % (iter, t, delta_phi))\n",
    "    \n",
    "    # Verify the Armijo condition\n",
    "    ### TODO BEGIN ###\n",
    "    # if ...\n",
    "      exitflag = 0\n",
    "      break\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Stop when the maximum number of iterations has been reached\n",
    "    elif iter >= max_iterations:\n",
    "      exitflag = 1\n",
    "      print('Warning: Armijo is stopping because the maximum number of iterations is reached.\\n')\n",
    "      break\n",
    "    # Stop when the function appears locally constant and the initial step length has decreased significantly\n",
    "    elif (delta_phi == 0) and (t / initial_t < 1e-12): \n",
    "      exitflag = 2                               \n",
    "      if verbosity == 'verbose':\n",
    "        print('Warning: Armijo is stopping because the function appears locally constant.\\n')\n",
    "      break\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Reduce the trial step size and increase the counter\n",
    "    # t = ...\n",
    "    # iter = ...\n",
    "    ### TODO END ###\n",
    "    \n",
    "  # Check whether the step length is in fact positive\n",
    "  if t < 0.0:\n",
    "    raise ValueError('Armijio is returning a negative step length.')\n",
    "  else:\n",
    "    return t, exitflag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lokale Minimierer\n",
    "\n",
    "Das Gradientenverfahren arbeitet mit lokalen Informationen.\n",
    "Für nichtquadratische Funktionen, z.B. welche die mehrere stationäre Punkte haben, können wir nicht vorhersagen ob bzw. zu welchem Punkt das Verfahren konvergieren wird.\n",
    "Im folgenden Skript soll das Gradientenverfahren für verschiedene Vorkonditionierer auf die Minimierung der Himmelblaufunktion angewendet und die Ergebnisse geplottet werden.\n",
    "\n",
    "**Aufgabe:** Implementieren Sie zwei weitere Vorkonditionierer, nämlich:\n",
    "1. Den Vorkonditionierer, der sich durch die Wahl der Hessematrix an der Startiterierten ergibt. (Der erste Schritt ist also ein Newton-Schritt)\n",
    "1. Den Vorkonditionierer, der sich durch die Wahl der Hessematrix an einem Minimierer (3,2) ergibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from objective_functions import *\n",
    "from visualization_functions import *\n",
    "\n",
    "# Create problem data\n",
    "f = himmelblau\n",
    "x0 = np.array([8.0, 0.0])\n",
    "\n",
    "# Set parameters for armijo linesearch\n",
    "armijo_parameters = {\n",
    "\"sigma\" : 0.01,\n",
    "\"beta\" : 0.5,\n",
    "\"initial_step_length\" : 2,\n",
    "#\"verbosity\" : \"verbose\"\n",
    "}\n",
    "\n",
    "# Construct step length rule\n",
    "armijo_step_length_rule = lambda phi, reusables: armijo_backtracking(phi, reusables, armijo_parameters);\n",
    "\n",
    "# Construct the preconditioners\n",
    "preconditioners = [(np.identity(len(x0)), \"Identity\"),\n",
    "                   ### TODO BEGIN ###\n",
    "                   # Add two preconditioners of the format\n",
    "                   # (matrix, \"Label for plotting\"),\n",
    "                   ### TODO END ###\n",
    "                  ]\n",
    "\n",
    "# Set gradient scheme parameters\n",
    "optimization_parameters = {\n",
    "\"atol_x\" : 1e-7,\n",
    "\"rtol_x\" : 1e-7,\n",
    "\"atol_f\" : 1e-7,\n",
    "\"rtol_f\" : 1e-14,\n",
    "\"max_iterations\" : 1e4,\n",
    "\"c\" : 10,\n",
    "\"verbosity\" : \"verbose\",\n",
    "\"keep_history\" : True\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "labels = []\n",
    "\n",
    "# Solve problem for all preconditioners\n",
    "for preconditioner, label in preconditioners:\n",
    "  outputs.append(gradient_descent(f, x0, armijo_step_length_rule, preconditioner, optimization_parameters))\n",
    "  labels.append(label)\n",
    "    \n",
    "# Plot history in iterate space\n",
    "plot_2d_iterates_contours(f, list(out[\"history\"] for out in outputs), labels)\n",
    "\n",
    "# Plot functional value differences (approximation of error energy norm)\n",
    "plot_f_val_diffs(list(out[\"history\"] for out in outputs), \n",
    "                list(out[\"history\"][\"objective_values\"][-1] for out in outputs),\n",
    "                labels)\n",
    "\n",
    "plot_step_sizes(list(out[\"history\"] for out in outputs), labels)\n",
    "\n",
    "plot_grad_norms(list(out[\"history\"] for out in outputs), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Beschreiben Sie Ihre Beobachtungen zum Verhalten des obigen Beispiels. Variieren Sie auch den Startpunkt $x_0$ (Zeile 12) und erklären Sie, warum die Wahl der Hessematrix an der Startiterierten keine sonderlich gute Idee ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Ihre Antwort hier.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
