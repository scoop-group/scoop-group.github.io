{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmierübung 1 zu *Grundlagen der Optimierung* (WS2023)\n",
    "\n",
    "## Einführung\n",
    "\n",
    "### Verantwortlich\n",
    "* Dr. Evelyn Herberg\n",
    "* M.Sc. Masoumeh Hashemi\n",
    "* B.Sc. Viktor Stein\n",
    "\n",
    "### Zielsetzung\n",
    "Das Ziel dieses *Jupyter Notebooks* ist es, Ihnen das Verhalten der im Kapitel 1 des Skripts vorgestellten Algorithmen zur Lösung allgemeiner, unrestringierter Optimierungsprobleme nahezubringen.\n",
    "Wir werden die Vorteile des *Newton-Verfahrens* gegenüber dem Gradientenverfahren herausarbeiten.\n",
    "\n",
    "### Zur Nutzung des Notebooks\n",
    "Die numerische Umsetzung der Verfahren und die graphische Visualisierung typischer Ergebnisse ist ein essentieller Baustein auf dem Weg zu einem ausgereiften Verständnis der Algorithmen.\n",
    "Um programmiertechnische Schwierigkeiten weitestgehend auszuschließen, haben wir einiges an Code für Sie vorbereitet.\n",
    "An den Schlüsselstellen der jeweiligen Implementierungen wurde der lauffähige Code durch auskommentierte Blöcke der Art\n",
    "```python\n",
    "### TODO BEGIN ###\n",
    "# Compute the preconditioned gradient and the square of its (preconditioner-induced) norm \n",
    "# gradient = ...\n",
    "# norm2_gradient = ...\n",
    "### TODO END ###\n",
    "```\n",
    "ersetzt, in denen Sie zwischen `### TODO BEGIN ###` und `### TODO END ###` die entsprechenden Anweisungen Variablen (in diesem Beispiel die Berechnung von `gradient` und `norm2_gradient`) entsprechend des Kommentars ausführen, um Lauffähigkeit wieder herzustellen.\n",
    "Welche Berechnungen und Auswertungen an den jeweiligen Stellen benötigt werden, können Sie im Skript nachlesen.\n",
    "Sie können natürlich, bevor Sie genau diese vorbenannten Variablen schreiben, auch eigene Variablen beschreiben.\n",
    "\n",
    "Wenn sie den Code vervollständigt haben, werden Sie an geeigneter Stelle um die Interpretation der Ergebnisse gebeten.\n",
    "In den entsprechenden Zellen ersetzen Sie \"**TODO Ihre Antwort hier**\" mit ihrer Antwort.\n",
    "\n",
    "Notwendige Lösungen aus der ersten Programmieraufgabe werden ihnen im Folgenden bereitgestellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module implements the preconditioned gradient scheme.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, x0, step_length_rule, preconditioner, parameters = {}):\n",
    "  \"\"\" \n",
    "  Solve an unconstrained minimization problem using the preconditioned\n",
    "  gradient descent method.\n",
    "\n",
    "  Accepts:  \n",
    "                   f: the objective function to be minimized\n",
    "                  x0: the initial guess (list or numpy array with ndim == 1)\n",
    "    step_length_rule: a step length computation function \n",
    "      preconditioner: a symmetric positive definite matrix (numpy array with ndim == 2)\n",
    "          parameters: optional parameters (dictionary);\n",
    "                      the following key/value pairs are evaluated:\n",
    "                                [\"atol_x\"]: absolute stopping tolerance for the norm of updates in x\n",
    "                                [\"rtol_x\"]: relative stopping tolerance for the norm of updates in x\n",
    "                                [\"atol_f\"]: absolute stopping tolerance for the progress in the values of f\n",
    "                                [\"rtol_f\"]: relative stopping tolerance for the progress in the values of f\n",
    "                            [\"atol_gradf\"]: absolute stopping tolerance for the norm of the gradient of f\n",
    "                            [\"rtol_gradf\"]: relative stopping tolerance for the norm of the gradient of f\n",
    "                        [\"max_iterations\"]: maximum number of iterations\n",
    "                             [\"verbosity\"]: \"verbose\" or \"quiet\"\n",
    "                          [\"keep_history\"]: whether or not to store the iteration history (True or False) \n",
    "                      Here 'norm' refers to the preconditioner-induced norm.\n",
    "    \n",
    "  Returns: \n",
    "              result: a dictionary containing \n",
    "                          solution: final iterate\n",
    "                          function: the final iterate's objective value \n",
    "                          gradient: the final iterate's objective gradient value\n",
    "                     norm_gradient: preconditioner-induced norm of final objective gradient \n",
    "                              iter: number of iterations performed\n",
    "                          exitflag: flag encoding why the algorithm terminated\n",
    "                                   0: stopping tolerance described by atol_x, rtol_x, atol_f, rtol_f reached\n",
    "                                   1: stopping tolerance described by atol_gradf and rtol_gradf reached\n",
    "                                   2: maximum number of iterations reached\n",
    "\n",
    "                       history: a dictionary for the history of the run containing\n",
    "                                          iterates: the iterates x\n",
    "                                  objective_values: the values of the objective function\n",
    "                                    gradient_norms: the norms of the objective function gradient \n",
    "                                     steps_lengths: the step lengths chosen by the step length rule\n",
    "  \"\"\"\n",
    "  # Define computation of the squared preconditioner norm\n",
    "  def norm2(d): return d.dot(preconditioner.dot(d))\n",
    "\n",
    "  # Define an output function that will be used to print information on the state of the iteration\n",
    "  def print_header(): \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(' ITER          OBJ    NORM_GRAD    NORM_CORR     OBJ_CHNG           ')\n",
    "    print('--------------------------------------------------------------------')\n",
    "  \n",
    "  # Define exitflags messages that will be printed when the algorithm terminates\n",
    "  exitflag_messages = [\n",
    "      'Relative and absolute tolerances on the norm of the update and the descent of the objective are satisfied.',\n",
    "      'Relative and absolute tolerances on the norm of the gradient are satisfied.',\n",
    "      'Maximum number of optimization steps is reached.',\n",
    "      ]\n",
    "  \n",
    "  # Get the algorithmic parameters, using defaults if missing\n",
    "  atol_x = parameters.get(\"atol_x\", 1e-6)\n",
    "  rtol_x = parameters.get(\"rtol_x\", 1e-6)\n",
    "  atol_f = parameters.get(\"atol_f\", 1e-6)\n",
    "  rtol_f = parameters.get(\"rtol_f\", rtol_x**2)\n",
    "  atol_gradf = parameters.get(\"atol_gradf\", 1e-6)\n",
    "  rtol_gradf = parameters.get(\"rtol_gradf\", 1e-6)\n",
    "  max_iterations = parameters.get(\"max_iterations\", 1e3)\n",
    "  verbosity = parameters.get(\"verbosity\", \"quiet\")\n",
    "  keep_history = parameters.get(\"keep_history\", False)\n",
    "\n",
    "  # Initialize the iterates, counters etc.\n",
    "  x = x0\n",
    "  iter = 0\n",
    "  exitflag = None\n",
    "  \n",
    "  # Initialize dummy values pertaining to the previous iterate\n",
    "  x_old = np.full(x0.shape, np.inf)\n",
    "  function_value_old = np.inf\n",
    "\n",
    "  # Prepare a dictionary to store the history\n",
    "  if keep_history:\n",
    "    history = {\n",
    "      \"iterates\" : [],\n",
    "      \"objective_values\" : [],\n",
    "      \"gradient_norms\" : [],\n",
    "      \"step_lengths\" : []\n",
    "      }\n",
    "  \n",
    "  # Perform gradient descent steps until one of the termination criteria is met\n",
    "  while exitflag is None:\n",
    "    # Record the current iterate\n",
    "    if keep_history: history[\"iterates\"].append(x)\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose':\n",
    "      if (iter%10 == 0): print_header()\n",
    "      print(' %4d  ' % (iter), end = '')\n",
    "            \n",
    "    # Stop when the maximum number of iterations has been reached\n",
    "    if iter >= max_iterations:\n",
    "      exitflag = 2\n",
    "      break\n",
    "    \n",
    "    # Compute the function value and derivative at current iterate\n",
    "    values = f(x, derivatives = [True, True, False])\n",
    "    function_value = values[\"function\"]\n",
    "    derivative = values[\"derivative\"]\n",
    "    \n",
    "    # Record the current value of the objective\n",
    "    if keep_history: history[\"objective_values\"].append(function_value)\n",
    "        \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  ' % (function_value), end = '')\n",
    "    \n",
    "    # Compute the preconditioned gradient and the square of its (preconditioner-induced) norm\n",
    "    gradient = np.linalg.solve(preconditioner, derivative)\n",
    "    norm2_gradient = derivative.dot(gradient)\n",
    "    \n",
    "    # Check the computed norm square for positivity\n",
    "    if norm2_gradient < 0:\n",
    "      raise ValueError('Your preconditioner appears not to be positive definite.')\n",
    "    else:\n",
    "      norm_gradient = np.sqrt(norm2_gradient)\n",
    "    \n",
    "    # Record the current norm of the gradient\n",
    "    if keep_history: history[\"gradient_norms\"].append(norm_gradient)\n",
    "\n",
    "    # Remember the norm of the initial gradient\n",
    "    if (iter == 0): initial_norm_gradient = norm_gradient\n",
    "        \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  ' % (norm_gradient), end = '')\n",
    "    \n",
    "    # Stop when the stopping tolerance on the norm of the gradient has been reached\n",
    "    if norm_gradient <= atol_gradf + rtol_gradf * initial_norm_gradient:\n",
    "      exitflag = 1\n",
    "      break\n",
    "    \n",
    "    # Evaluate the norm of the update step\n",
    "    norm_delta_x = np.sqrt(norm2(x - x_old))\n",
    "    \n",
    "    # Evaluate the change in the objective function values\n",
    "    delta_f = function_value_old - function_value\n",
    "    \n",
    "    # Evaluate the reference values for relative tolerances\n",
    "    abs_function_value_old = np.abs(function_value_old)\n",
    "    norm_x_old = np.sqrt(norm2(x_old))\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  %11.4e' % (norm_delta_x, -delta_f))\n",
    "    \n",
    "    # Stop when the stopping tolerance on the change in the objective and the\n",
    "    # norm of the update step have been reached\n",
    "    if (delta_f < atol_f + rtol_f * abs_function_value_old) and\\\n",
    "      (norm_delta_x < atol_x + rtol_x * norm_x_old):\n",
    "      exitflag = 0\n",
    "      break\n",
    "    \n",
    "    # Set the update direction\n",
    "    d = -gradient\n",
    "    \n",
    "    # Prepare the line search function, using the function values of the\n",
    "    # objective and its derivatives and the chain rule\n",
    "    def phi(t, derivatives):\n",
    "      values = f(x + t * d, derivatives)\n",
    "      if derivatives[1]:\n",
    "        values[\"derivative\"] = values[\"derivative\"].dot(d)\n",
    "      if derivatives[2]:\n",
    "        values[\"Hessian\"] = d.dot(values[\"Hessian\"].dot(d))\n",
    "      return values\n",
    "    \n",
    "    # Prepare some data to pass down to the step length computation rule\n",
    "    reusables = {\n",
    "      \"phi0\" : function_value,\n",
    "      \"dphi0\" : -norm2_gradient\n",
    "      }\n",
    "    \n",
    "    # Evaluate the step length t using the step length rule\n",
    "    t, t_exitflag = step_length_rule(phi, reusables)\n",
    "    \n",
    "    # Check whether of not the step length was computed succesfully\n",
    "    if t_exitflag: raise AssertionError('Step length was not computed succesfully.')\n",
    "    \n",
    "    # Record the chosen step length\n",
    "    if keep_history: history[\"step_lengths\"].append(t)\n",
    "    \n",
    "    # Save the current iterate and associated function value for the next iteration\n",
    "    x_old = x\n",
    "    function_value_old = function_value\n",
    "    \n",
    "    # Update the iterate and increase the counter\n",
    "    x = x + t * d\n",
    "    iter = iter + 1\n",
    "\n",
    "  # Dump some output\n",
    "  if verbosity == 'verbose':\n",
    "    print('\\n\\nThe gradient descent method exiting with flag %d.\\n' %(exitflag) + str(exitflag_messages[exitflag])+'\\n' )\n",
    "  \n",
    "  # Create and populate the result to be returned\n",
    "  result = {\n",
    "    \"solution\" : x,\n",
    "    \"function\" : function_value,\n",
    "    \"gradient\" : gradient,\n",
    "    \"norm_gradient\" : norm_gradient,\n",
    "    \"iter\" : iter,\n",
    "    \"exitflag\" : exitflag\n",
    "    }\n",
    "\n",
    "  # Assign the history to the result if required\n",
    "  if keep_history:\n",
    "    result[\"history\"] = history\n",
    "      \n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo_backtracking(phi, reusables = {}, parameters = {}):\n",
    "  \"\"\"\n",
    "  Compute a step length t via backtracking satisfying the Armijo condition \n",
    "  for the function phi, i.e.,\n",
    "\n",
    "    phi(t) <= phi(0) + sigma * t * dphi(0) \n",
    "\n",
    "  where dphi(0) is the derivative of phi at t = 0.\n",
    "\n",
    "  Accepts: \n",
    "           phi: evaluates the function the line search is performed on\n",
    "     reusables: additional information that may be provided to the method (dictionary);\n",
    "                the following key/value pairs are evaluated:\n",
    "                   [\"phi0\"]: the value of phi at t = 0 (scalar)\n",
    "                  [\"dphi0\"]: the value of the derivative of phi at t = 0 (scalar)\n",
    "\n",
    "  Returns:\n",
    "            t: the step length minimizing phi (provided it is quadratic)\n",
    "     exitflag: flag encoding why the line search terminated\n",
    "                 0: success\n",
    "                 1: maximum number of iterations reached\n",
    "                 2: trial step length became too small\n",
    "  \"\"\"\n",
    "\n",
    "  def print_header():\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(' ARMIJO:     ITER          STEP      OBJCHNG           ')\n",
    "    print('--------------------------------------------------------------------')\n",
    "  \n",
    "  # Get the line search parameters, using defaults if missing\n",
    "  sigma = parameters.get(\"sigma\", 0.01)\n",
    "  beta = parameters.get(\"beta\", 0.5)\n",
    "  initial_t = parameters.get(\"initial_step_length\", 1.0)\n",
    "  verbosity = parameters.get(\"verbosity\", \"quiet\")\n",
    "  max_iterations = parameters.get(\"max_iterations\", 1e4)\n",
    "    \n",
    "  # Extract or compute required data for checking armijo condition\n",
    "  phi0 = reusables.get(\"phi0\", phi(0, derivatives = [True, False, False])[\"function\"]) or\\\n",
    "    phi(0, derivatives[True,False,False])[\"function\"]\n",
    "  dphi0 = reusables.get(\"dphi0\", phi(0, derivatives = [False, True, False])[\"derivative\"]) or\\\n",
    "    phi(0, derivatives[False,True,False])[\"derivative\"]\n",
    "  \n",
    "  if dphi0 >= 0:\n",
    "    raise(InputError('The function phi is expected to be decreasing at zero..'))\n",
    "  \n",
    "  # Initialize the step length and counter\n",
    "  t = initial_t\n",
    "  iter = 0\n",
    "  exitflag = None\n",
    "    \n",
    "  # Perform the backtracking search until one of the termination criteria is met\n",
    "  while exitflag is None:\n",
    "\n",
    "    # Evaluate the value of phi at the current trial step length and the amount of descent\n",
    "    phi_trial = phi(t, derivatives = [True, False, False])[\"function\"]\n",
    "    delta_phi = phi_trial - phi0\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose':\n",
    "      if (iter%10 == 0): print_header()\n",
    "      print('             %4d   %11.4e  %11.4e  \\n' % (iter, t, delta_phi))\n",
    "    \n",
    "    # Verify the Armijo condition\n",
    "    if delta_phi <= sigma * t * dphi0:\n",
    "      exitflag = 0\n",
    "      break\n",
    "    # Stop when the maximum number of iterations has been reached\n",
    "    elif iter >= max_iterations:\n",
    "      exitflag = 1\n",
    "      print('Warning: Armijo is stopping because the maximum number of iterations is reached.\\n')\n",
    "      break\n",
    "    # Stop when the function appears locally constant and the initial step length has decreased significantly\n",
    "    elif (delta_phi == 0) and (t / initial_t < 1e-12): \n",
    "      exitflag = 2                               \n",
    "      if verbosity == 'verbose':\n",
    "        print('Warning: Armijo is stopping because the function appears locally constant.\\n')\n",
    "      break\n",
    "    \n",
    "    # Reduce the trial step size and increase the counter\n",
    "    t = beta * t\n",
    "    iter = iter + 1\n",
    "    \n",
    "  # Check whether the step length is in fact positive\n",
    "  if t < 0.0:\n",
    "    raise ValueError('Armijio is returning a negative step length.')\n",
    "  else:\n",
    "    return t, exitflag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das globalisierte Newton-Verfahren\n",
    "\n",
    "Die Effekte, die sie in dem Beispiel der Himmelblaufunktion beobachtet haben zeigen, dass die unreflektierte Verwendung der Hessematrix als \"adaptiver Vorkonditionierer\" nicht funktionieren kann - daher verwendet man beim Newton-Verfahren die im Skript beschriebene globalisierte Variante.\n",
    "Dass dieses Verfahren gegenüber dem Gradientenverfahren eine massive Verbesserung der Konvergenzgeschwindigkeit liefern kann, möchten wir anhand der [Rosenbrock Funktion](https://en.wikipedia.org/wiki/Rosenbrock_function) verifizieren.\n",
    "\n",
    "### Implementierung des globalisierten Newton-Verfahrens\n",
    "**Aufgabe:** Vervollständigen Sie den Code in der nächsten Zelle und führen Sie die Zelle aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1686965772.py, line 148)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 148\u001b[0;36m\u001b[0m\n\u001b[0;31m    exitflag = 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def globalized_newton(f, x0, step_length_rule, preconditioner, parameters = {}):\n",
    "  \"\"\"\n",
    "  Solve an unconstrained minimization problem using the line search globalized newton method.\n",
    "  \n",
    "  Accepts:  \n",
    "                   f: the objective function to be minimized\n",
    "                  x0: the initial guess (list or numpy array with ndim == 1)\n",
    "    step_length_rule: a step length computation function \n",
    "      preconditioner: a symmetric positive definite matrix (numpy array with ndim == 2)\n",
    "          parameters: optional parameters (dictionary);\n",
    "                      the following key/value pairs are evaluated:\n",
    "                                    [\"atol_x\"]: absolute stopping tolerance for the norm of updates in x\n",
    "                                    [\"rtol_x\"]: relative stopping tolerance for the norm of updates in x\n",
    "                                    [\"atol_f\"]: absolute stopping tolerance for the progress in the values of f\n",
    "                                    [\"rtol_f\"]: relative stopping tolerance for the progress in the values of f\n",
    "                                [\"atol_gradf\"]: absolute stopping tolerance for the norm of the gradient of f\n",
    "                                [\"rtol_gradf\"]: relative stopping tolerance for the norm of the gradient of f\n",
    "                            [\"max_iterations\"]: maximum number of iterations\n",
    "                                      [\"rho1\"]: globalization parameter\n",
    "                                      [\"rho2\"]: globalization parameter\n",
    "                                         [\"p\"]: globalization parameter\n",
    "                                 [\"verbosity\"]: \"verbose\" or \"quiet\"\n",
    "                              [\"keep_history\"]: whether or not to store the iteration history (True or False) \n",
    "                      Here 'norm' refers to the preconditioner-induced norm.\n",
    "    \n",
    "  Returns: \n",
    "              result: a dictionary containing \n",
    "                       solution: final iterate\n",
    "                       function: the final iterate's objective value \n",
    "                       gradient: the final iterate's objective gradient value\n",
    "                      norm_gradient: preconditioner-induced norm of final objective gradient \n",
    "                           iter: number of iterations performed\n",
    "                       exitflag: flag encoding why the algorithm terminated\n",
    "                                  0: stopping tolerance described by atol_x, rtol_x, atol_f, rtol_f reached\n",
    "                                  1: stopping tolerance described by atol_gradf and rtol_gradf reached\n",
    "                                  2: maximum number of iterations reached\n",
    "\n",
    "                        history: a dictionary for the history of the run containing\n",
    "                                              iterates: the iterates x\n",
    "                                      objective_values: the values of the objective function\n",
    "                                        gradient_norms: the norms of the objective function gradient \n",
    "                                         steps_lengths: the step lengths chosen by the step length rule\n",
    "                                 used_newton_direction: the information whether or not the newton direction was used\n",
    "  \"\"\"\n",
    "  # Define computation of the squared preconditioner norm\n",
    "  def norm2(d): return d.dot(preconditioner.dot(d))\n",
    "  \n",
    "  # Define an output function that will be used to print information on the state of the iteration\n",
    "  def print_header():\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(' ITER          OBJ    NORM_GRAD    NORM_CORR     OBJ_CHNG           ')\n",
    "    print('--------------------------------------------------------------------')\n",
    "  \n",
    "  # Define exitflags that will be printed when the algorithm terminates\n",
    "  exitflag_messages = [\n",
    "      'Relative and absolute tolerances on the norm of the update and the descent of the objective are satisfied.',\n",
    "      'Relative and absolute tolerances on the norm of the gradient are satisfied.',\n",
    "      'Maximum number of optimization steps is reached.',\n",
    "      ]\n",
    "  \n",
    "  # Get the algorithmic parameters, using defaults if missing\n",
    "  atol_x = parameters.get(\"atol_x\", 1e-6)\n",
    "  rtol_x = parameters.get(\"rtol_x\", 1e-6)\n",
    "  atol_f = parameters.get(\"atol_f\", 1e-6)\n",
    "  rtol_f = parameters.get(\"rtol_f\", rtol_x**2)\n",
    "  atol_gradf = parameters.get(\"atol_gradf\", 1e-6)\n",
    "  rtol_gradf = parameters.get(\"rtol_gradf\", 1e-6)\n",
    "  max_iterations = parameters.get(\"max_iterations\", 1e3)\n",
    "  rho1 = parameters.get(\"rho1\", 1e-6)\n",
    "  rho2 = parameters.get(\"rho2\", 1e-6)\n",
    "  p = parameters.get(\"p\", 0.1)\n",
    "  verbosity = parameters.get(\"verbosity\", \"quiet\")\n",
    "  keep_history = parameters.get(\"keep_history\", False)\n",
    "\n",
    "  # Initialize the iterates, counters etc.\n",
    "  x = x0\n",
    "  iter = 0\n",
    "  exitflag = None\n",
    "  used_newton_direction = None\n",
    "  \n",
    "  # Initialize dummy values pertaining to the previous iterate\n",
    "  x_old = np.full(x0.shape, np.inf)\n",
    "  function_value_old = np.inf\n",
    "\n",
    "  # Prepare a dictionary to store the history\n",
    "  if keep_history:\n",
    "    history = {\n",
    "      \"iterates\" : [],\n",
    "      \"objective_values\" : [],\n",
    "      \"gradient_norms\" : [],\n",
    "      \"step_lengths\" : [],\n",
    "      \"used_newton_direction\" : []\n",
    "      }\n",
    "  \n",
    "  # Perform gradient descent steps until one of the termination criteria is met\n",
    "  while exitflag is None:\n",
    "    # Record the current iterate\n",
    "    if keep_history: history[\"iterates\"].append(x)\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose':\n",
    "      if (iter%10 == 0): print_header()\n",
    "      print(' %4d  ' % (iter), end = '')\n",
    "            \n",
    "    # Stop when the maximum number of iterations has been reached\n",
    "    if iter >= max_iterations:\n",
    "      exitflag = 2\n",
    "      break\n",
    "    \n",
    "    # Compute the function value and derivative at current iterate\n",
    "    values = f(x, derivatives = [True, True, True])\n",
    "    function_value = values[\"function\"]\n",
    "    derivative = values[\"derivative\"]\n",
    "    hessian = values[\"Hessian\"]\n",
    "    \n",
    "    # Record the current value of the objective\n",
    "    if keep_history: history[\"objective_values\"].append(function_value)\n",
    "        \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  ' % (function_value), end = '')\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Compute the preconditioned gradient and the square of its (preconditioner-induced) norm \n",
    "    # gradient = ...\n",
    "    # norm2_gradient = ...\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Check the computed norm square for positivity\n",
    "    if norm2_gradient < 0:\n",
    "      raise ValueError('Your preconditioner appears not to be positive definite.')\n",
    "    else:\n",
    "      norm_gradient = np.sqrt(norm2_gradient)\n",
    "    \n",
    "    # Record the current norm of the gradient\n",
    "    if keep_history: history[\"gradient_norms\"].append(norm_gradient)\n",
    "\n",
    "    # Remember the norm of the initial gradient\n",
    "    if (iter == 0): initial_norm_gradient = norm_gradient\n",
    "        \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  ' % (norm_gradient), end = '')\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Stop when the stopping tolerance on the norm of the gradient has been reached\n",
    "    # if ...\n",
    "      exitflag = 1\n",
    "      break\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Evaluate the norm of the update step\n",
    "    norm_delta_x = np.sqrt(norm2(x - x_old))\n",
    "    \n",
    "    # Evaluate the change in the objective function values\n",
    "    delta_f = function_value_old - function_value\n",
    "    \n",
    "    # Evaluate the reference values for relative tolerances\n",
    "    abs_function_value_old = np.abs(function_value_old)\n",
    "    norm_x_old = np.sqrt(norm2(x_old))\n",
    "    \n",
    "    # Dump some output\n",
    "    if verbosity == 'verbose': print('%11.4e  %11.4e' % (norm_delta_x, -delta_f))\n",
    "    \n",
    "    # Stop when the stopping tolerance on the change in the objective and the\n",
    "    # norm of the update step have been reached\n",
    "    if (delta_f < atol_f + rtol_f * abs_function_value_old) and\\\n",
    "      (norm_delta_x < atol_x + rtol_x * norm_x_old):\n",
    "      exitflag = 0\n",
    "      break\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Set the update direction\n",
    "    try:      \n",
    "      # Compute Newton direction and its (preconditioner induced) norm\n",
    "      # d = ...\n",
    "      \n",
    "      # Check Newton direction for quality\n",
    "      # if ...\n",
    "      used_newton_direction = True\n",
    "    except:\n",
    "      # Fall back to gradient direction if Newton direction could not be computed or is of poor quality\n",
    "      used_newton_direction = False\n",
    "      # d = ...\n",
    "    ### TODO END ###\n",
    "    \n",
    "    # Prepare the line search function, using the function values of the\n",
    "    # objective and its derivatives and the chain rule\n",
    "    def phi(t, derivatives):\n",
    "      values = f(x + t * d, derivatives)\n",
    "      if derivatives[1]:\n",
    "        values[\"derivative\"] = values[\"derivative\"].dot(d)\n",
    "      if derivatives[2]:\n",
    "        values[\"Hessian\"] = d.dot(values[\"Hessian\"].dot(d))\n",
    "      return values\n",
    "    \n",
    "    # Prepare some data to pass down to the step length computation rule\n",
    "    reusables = {\n",
    "      \"phi0\" : function_value,\n",
    "      }\n",
    "    if not used_newton_direction: reusables[\"dphi0\"] = -norm2_gradient\n",
    "    \n",
    "    # Evaluate the step length t using the step length rule\n",
    "    t, t_exitflag = step_length_rule(phi, reusables)\n",
    "    \n",
    "    # Check whether of not the step length was computed succesfully\n",
    "    if t_exitflag: raise AssertionError('Step length was not computed succesfully.')\n",
    "    \n",
    "    # Record the chosen step length \n",
    "    if keep_history: history[\"step_lengths\"].append(t)\n",
    "    \n",
    "    # Save the current iterate and associated function value for the next iteration\n",
    "    x_old = x\n",
    "    function_value_old = function_value\n",
    "    \n",
    "    ### TODO BEGIN ###\n",
    "    # Update the iterate and increase the counter\n",
    "    # x = ...\n",
    "    # iter = ...\n",
    "    ### TODO END ###\n",
    "\n",
    "    # Remember whether or not the newton direction was used as update direction\n",
    "    if keep_history: history[\"used_newton_direction\"].append(used_newton_direction)\n",
    "\n",
    "  # Dump some output\n",
    "  if verbosity == 'verbose':\n",
    "    print('\\n\\nThe globalized newton is exiting with flag %d.\\n' %(exitflag) + str(exitflag_messages[exitflag])+'\\n' )\n",
    "  \n",
    "  # Create and populate the result to be returned\n",
    "  result = {\n",
    "    \"solution\" : x,\n",
    "    \"function\" : function_value,\n",
    "    \"gradient\" : gradient,\n",
    "    \"norm_gradient\" : norm_gradient,\n",
    "    \"iter\" : iter,\n",
    "    \"exitflag\" : exitflag\n",
    "    }\n",
    "\n",
    "  # Assign the history to the result if required\n",
    "  if keep_history:\n",
    "    result[\"history\"] = history\n",
    "      \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Skript wird das globalisierte Newton-Verfahren im Vergleich zum euklidisch vorkonditionierten Gradientenverfahren für die Minimierung der Rosenbrock Funktion verwendet.\n",
    "\n",
    "**Aufgabe:** Vervollständigen Sie das Skript und führen Sie es aus. Beachten Sie die Bedingungen, die im Skript an die Parameter gestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from objective_functions import *\n",
    "from visualization_functions import *\n",
    "\n",
    "# Create problem data\n",
    "rosenbrock_parameters = {\n",
    "    \"a\" : 1,\n",
    "    \"b\" : 10,\n",
    "  }\n",
    "f = lambda x, derivatives: rosenbrock(x, derivatives, rosenbrock_parameters)\n",
    "x0 = np.array([-1.75, 1.5])\n",
    "\n",
    "### TODO BEGIN ###\n",
    "# Set parameters for armijo linesearch\n",
    "# armijo_parameters = {\n",
    "# \"sigma\" : ...,\n",
    "# \"beta\" : ...,\n",
    "# \"initial_step_length\" : ...\n",
    "#}\n",
    "### TODO END ###\n",
    "\n",
    "if not armijo_parameters[\"sigma\"] < 0.5:\n",
    "  raise ValueError('Chosen armijo sigma is not smaller than 0.5')\n",
    "if not armijo_parameters[\"initial_step_length\"] == 1:\n",
    "  raise ValueError('Initial step length is not 1.')\n",
    "\n",
    "# Construct step length rule\n",
    "armijo_step_length_rule = lambda phi, reusables: armijo_backtracking(phi, reusables, armijo_parameters);\n",
    "\n",
    "# Set optimization parameters\n",
    "optimization_parameters = {\n",
    "\"atol_x\" : 1e-7,\n",
    "\"rtol_x\" : 1e-7,\n",
    "\"atol_f\" : 1e-7,\n",
    "\"rtol_f\" : 1e-14,\n",
    "\"max_iterations\" : 1e10,\n",
    "\"c\" : 10,\n",
    "\"verbosity\" : \"verbose\",\n",
    "\"keep_history\" : True\n",
    "}\n",
    "\n",
    "# Solve problem using preconditioned gradient scheme\n",
    "gradient_output = gradient_descent(f, x0, armijo_step_length_rule, preconditioner = np.identity(len(x0)), parameters = optimization_parameters)\n",
    "gradient_label = \"Gradient\"\n",
    "  \n",
    "# Solve problem using globalized Newton scheme\n",
    "newton_output = globalized_newton(f, x0, armijo_step_length_rule, preconditioner = np.identity(len(x0)), parameters = optimization_parameters)\n",
    "newton_label = \"Newton\"\n",
    "\n",
    "# Prepare data for plotting\n",
    "histories = [gradient_output[\"history\"], newton_output[\"history\"]]\n",
    "labels = [gradient_label, newton_label]\n",
    "\n",
    "# Plot history in iterate space\n",
    "plot_2d_iterates_contours(f, histories, labels)\n",
    "\n",
    "# Plot functional value differences (approximation of error energy norm)\n",
    "plot_f_val_diffs(histories, \n",
    "                list(hist[\"objective_values\"][-1] for hist in histories),\n",
    "                labels)\n",
    "\n",
    "plot_step_sizes([newton_output[\"history\"]], [newton_label])\n",
    "\n",
    "plot_grad_norms(histories, labels)\n",
    "\n",
    "plot_used_newton_direction([newton_output[\"history\"]], [newton_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Beschreiben Sie ihre Beobachtungen zum Konvergenzverhalten der beiden Verfahren.\n",
    "Warum eignet sich die Rosenbrock Funktion gut für diesen Vergleich? Bringt dem Gradientenverfahren die Wahl anderer Vorkonditionerer einen Vorteil?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Ihre Antwort hier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Wenn sie alle Funktionalitäten bis hier entwickeln konnten steht es Ihnen natürlich frei, ihre Lösung zu kopieren und in der Kopie beliebige Änderungen an den Parametern vorzunehmen. \n",
    "Eine genauerer Vergleich des Einflusses der Schrittweitenbestimmungen auf das Gradientenverfahren ist bspl. eine weitere interessante Untersuchung, die dem Verständnis der Parameter im Armijo hilfreich ist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
