<!DOCTYPE html>
<html lang="en-us"><head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="theme-color" content="white">
	<title> Multilevel Architectures and Algorithms in Deep Learning | Scientific Computing and Optimization</title>

	<link rel="canonical" href="https://scoop-group.github.io/projects/multilevel-architectures/">
  <link rel="alternate"
        type="application/rss+xml"
        href="https://scoop-group.github.io/index.xml"
        title="Scientific Computing and Optimization">


	
	

	
	<link rel="shortcut icon" type="image/png" href="https://scoop-group.github.io/img/logo.png" />
	
	<link rel="stylesheet" href="https://scoop-group.github.io/scss/style.css" integrity="">
	

	 <meta name="description" content="The research group Scientific Computing and Optimization was founded in 2021."> 
	<meta name="description" content="The research group Scientific Computing and Optimization was founded in 2021.">

	<meta name="robots" content="all,follow">
	<meta name="googlebot" content="index,follow,snippet,archive">
</head>
<body><header id="site-header">
	<a href="https://scoop-group.github.io">
		<img id="site-logo" src="https://scoop-group.github.io/img/logo.png" alt="logo">
	</a>
	<header id="site-title">
		<a href="https://scoop-group.github.io"> Scientific Computing and Optimization </a>
	</header>
	<nav id="site-menu">
		<ul>
			
				<li> <a href="../../news">News</a> </li>
			
				<li> <a href="../../team">Team</a> </li>
			
				<li> <a href="../../teaching">Teaching</a> </li>
			
				<li> <a href="../../publications">Publications</a> </li>
			
				<li> <a href="../../talks">Talks</a> </li>
			
				<li> <a href="../../software">Software</a> </li>
			
				<li> <a href="../../projects">Projects</a> </li>
			
				<li> <a href="../../events">Events</a> </li>
			
				<li> <a href="../../theses">Theses</a> </li>
			
				<li> <a href="../../contact">Contact</a> </li>
			
		</ul>
	</nav>
</header>
<div id="content">
<main>
	<header>

		<h1> Multilevel Architectures and Algorithms in Deep Learning
		</h1>
		<br>
		<div class="metadata">
		
	<span> Start: 2023-01-01</span>
	<br>



	<span> End: 2025-12-31</span>
	<br>



	<span> Principal Investigators: <span class="pis-list"><a href="../../team/rherzog/">Roland Herzog</a>, <a href="https://num.math.uni-bayreuth.de/de/team/anton-schiela/"target="_blank" rel="noopener" class="external">Anton Schiela</a></span></span>
	<br>





	<span> Staff: <span class="staff-list"><a href="../../team/lkreis/">Leonie Kreis</a>, <a href="https://num.math.uni-bayreuth.de/en/team/frederik-koehne/index.php"target="_blank" rel="noopener" class="external">Frederik Köhne</a></span></span>
	<br>



	<span>
	Funded by:
	<a href="https://www.dfg.de"target="_blank" rel="noopener" class="external">DFG</a>
			within the 
			Priority Program funding scheme
		
	</span>
	<br>



	<span> Part of: <a href="https://www.foundationsofdl.de/"target="_blank" rel="noopener" class="external">Theoretical Foundations of Deep Learning</a>
	
		(SPP 2298) </span>
	
	<br>

</div>
		
			<article>
				<h2>
					Project Description
				</h2>
				<nav id="TableOfContents"></nav>
				<p>The design of deep neural networks (DNNs) and their training is a central issue in machine learning.
Progress in these areas is one of the driving forces for the success of these technologies.
Nevertheless, tedious experimentation and human interaction is often still needed during the learning process to find an appropriate network structure and corresponding hyperparameters to obtain the desired behavior of a DNN.</p>
<p>The strategic goal of the proposed project is to provide algorithmic means to improve this situation.
Our methodical approach relies on well established mathematical techniques; identify fundamental algorithmic quantities and construct a-posteriori estimates for them, identify and consistently exploit an appropriate topological framework for the given problem class, establish a multilevel structure for DNNs to account for the fact that DNNs only realize a discrete approximation of a continuous nonlinear mapping relating input to output data.
Combining this idea with novel algorithmic control strategies and preconditioning, we will establish the new class of adaptive multilevel algorithms for deep learning, which not only optimize a fixed DNN, but also adaptively refine and extend the DNN architecture during the optimization loop.
This concept is not restricted to a particular network architecture, and we will study feedforward neural networks, ResNets, and PINNs as relevant examples.</p>
<p>Our integrated approach will thus be able to replace many of the current manual tuning techniques by algorithmic strategies, based on a-posteriori estimates.
Moreover, our algorithm will reduce the computational effort for training and also the size of the resulting DNN, compared to a manually designed counterpart, making the use of deep learning more efficient in many aspects.
Finally, in the long run our algorithmic approach has the potential to enhance the reliability and interpretability of the resulting trained DNN.</p>

			</article>
		

		
		
		
			
		
			
		
			
		
			
				
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
				
			
		
			
				
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
		
			
		
			
		
			
				
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
		
		
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		

		
			<h2> Associated Publications </h2>
<ul class="publications-list" id="publications-ARTICLE">
	<li class="publication-item" id="HerzogKoehneKreisSchiela:2025:1">
	<div class="publication-author"> <a href="../../team/rherzog/">Roland Herzog</a>, Frederik Köhne, <a href="../../team/lkreis/">Leonie Kreis</a> and Anton Schiela </div>
	
	
	
	<div class="publication-title"> Metric Frobenius norms and inner products of matrices and linear maps </div>
	<div class="publication-ref-info">
		
			<span> Linear Algebra and its Applications 727, p.112-128, </span>
		
		<span class="publication-year"> 2025 </span> <br>
	</div>
	
	
	
		<div class="publication-link">
			<a class="link-arXiv external" href="https://arxiv.org/abs/2311.15419">arXiv:2311.15419</a>
		</div>
	
	
	
	
	
	
		<div class="publication-link">
			<a class="link-doi external" href="https://doi.org/10.1016/j.laa.2025.08.005">doi:10.1016/j.laa.2025.08.005</a>
		</div>
	
	
	
	
		<details closed>
			<summary> bibtex </summary>
			<pre class="bibtex-cite">@ARTICLE{HerzogKoehneKreisSchiela:2025:1,
  AUTHOR = {Herzog, Roland and Köhne, Frederik and Kreis, Leonie and Schiela, Anton},
  DATE = {2025-08-08},
  DOI = {10.1016/j.laa.2025.08.005},
  EPRINT = {2311.15419},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {Linear Algebra and its Applications},
  PAGES = {112--128},
  TITLE = {Metric Frobenius norms and inner products of matrices and linear maps},
  VOLUME = {727},
}</pre>
		</details>
	
</li>
<li class="publication-item" id="HerbergHerzogKoehneKreisSchiela:2023:1">
	<div class="publication-author"> <a href="../../team/eherberg/">Evelyn Herberg</a>, <a href="../../team/rherzog/">Roland Herzog</a>, Frederik Köhne, <a href="../../team/lkreis/">Leonie Kreis</a> and Anton Schiela </div>
	
	
	
		<span class="publication-title"> Sensitivity-based layer insertion for residual and feedforward neural networks, </span>
		<span class="publication-year"> 2023 </span> <br>
	
	
		<div class="publication-link">
			<a class="link-arXiv external" href="https://arxiv.org/abs/2311.15995">arXiv:2311.15995</a>
		</div>
	
	
	
	
	
	
	
	
	
		<details closed>
			<summary> bibtex </summary>
			<pre class="bibtex-cite">@ONLINE{HerbergHerzogKoehneKreisSchiela:2023:1,
  AUTHOR = {Herberg, Evelyn and Herzog, Roland and Köhne, Frederik and Kreis, Leonie and Schiela, Anton},
  DATE = {2023-11},
  EPRINT = {2311.15995},
  EPRINTTYPE = {arXiv},
  TITLE = {Sensitivity-based layer insertion for residual and feedforward neural networks},
}</pre>
		</details>
	
</li>
<li class="publication-item" id="KoehneKreisSchielaHerzog:2023:1">
	<div class="publication-author"> Frederik Köhne, <a href="../../team/lkreis/">Leonie Kreis</a>, Anton Schiela and <a href="../../team/rherzog/">Roland Herzog</a> </div>
	
	
	
		<span class="publication-title"> Adaptive step sizes for preconditioned stochastic gradient descent, </span>
		<span class="publication-year"> 2023 </span> <br>
	
	
		<div class="publication-link">
			<a class="link-arXiv external" href="https://arxiv.org/abs/2311.16956">arXiv:2311.16956</a>
		</div>
	
	
	
	
	
	
	
	
	
		<details closed>
			<summary> bibtex </summary>
			<pre class="bibtex-cite">@ONLINE{KoehneKreisSchielaHerzog:2023:1,
  AUTHOR = {Köhne, Frederik and Kreis, Leonie and Schiela, Anton and Herzog, Roland},
  DATE = {2023-11},
  EPRINT = {2311.16956},
  EPRINTTYPE = {arXiv},
  TITLE = {Adaptive step sizes for preconditioned stochastic gradient descent},
}</pre>
		</details>
	
</li>
<li class="publication-item" id="Kreis:2022:1">
	<div class="publication-author"> <a href="../../team/lkreis/">Leonie Kreis</a> </div>
	
	
	
	<div class="publication-title"> Multilevel Training of Residual Neural Networks </div>
	<div class="publication-ref-info">
		
			<span> M.Sc. Thesis, Heidelberg University, </span>
		
		<span class="publication-year"> 2022 </span> <br>
	</div>
	
	
	
	
	
	
	
	
	
	
	
		<details closed>
			<summary> bibtex </summary>
			<pre class="bibtex-cite">@THESIS{Kreis:2022:1,
  AUTHOR = {Kreis, Leonie},
  INSTITUTION = {Heidelberg University},
  DATE = {2022-11-29},
  TITLE = {Multilevel Training of Residual Neural Networks},
  TYPE = {M.Sc. Thesis},
}</pre>
		</details>
	
</li>

</ul>


		
		
		<br>
		<div class="logo-container-single">
			<div class="logo-container-single-left">
			
 				
					<img  class="logo-project-single" src="../../img/agencylogos/DFG.png" alt="Agency logo">
				 
			
			</div>
			<div class="logo-container-single-right">
			
			
			</div>
		</div>
	</header>

</main>

        </div><footer>
	<span class="footer-info">
    <a href="https://creativecommons.org/licenses/by-nd/4.0/" class="footer-item">CC BY-ND</a>, Scientific Computing and Optimization, 2025
	</span>
	
	<span id="built">
		(site built 2025-12-02 08:58:05 UTC )
	</span>
	

	<a href="https://scoop-group.github.io/index.xml" id="rss"> [rss] </a>
</footer>
</body>
</html>
